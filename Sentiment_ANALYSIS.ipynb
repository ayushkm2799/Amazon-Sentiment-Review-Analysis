{"cells":[{"cell_type":"markdown","metadata":{"id":"Fy_r9FLpoNAo"},"source":["# General Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMVvY7x-lYZZ"},"outputs":[],"source":["#General Imports\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle \n","from os.path import join\n","import multiprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cTha2JRuHyF"},"outputs":[],"source":["# !pip uninstall numpy -y\n","# !pip install numpy"]},{"cell_type":"markdown","metadata":{"id":"TlN-2JHahxW7"},"source":["# Data Downsampling and Preprocessing\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO0m2xcEqJOu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660478758140,"user_tz":-330,"elapsed":1050,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"d97d4c47-2f34-4718-9a8e-304e134a6909"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["#Preprocessing related imports \n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","import gensim.parsing.preprocessing as gpp\n","import gensim.utils as gu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yt2Yqk2QipnP"},"outputs":[],"source":["#Load full dataset \n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/amazon_review_full_csv\"\n","data = pd.read_csv(join(data_dir, \"train.csv\"), header=None, names=['Rating', 'Title', 'Review'])\n","display(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNQLanndi-mw"},"outputs":[],"source":["# Check distribution of rating values as this is likely our target variable\n","data[\"Rating\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"sLSttV8_jN15"},"source":["\n","## Preprocessing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b-UCwDQajSY0"},"source":["We first preprocess our entire dataset by applying the following transformations to the textual data:\n","\n","    Stripping HTML Tags (gpp.strip_tags)\n","    Removing all Punctuation (gpp.strip_punctuation)\n","    Removing all extra whitespaces (gpp.strip_multiple_whitespaces)\n","    Removing all numerics (gpp.strip_numeric)\n","    Removing stopwords(gpp.remove_stopwords)\n","    Removing words shorter than 3 letters (gpp.strip_short)\n","\n","Following this initial pre-processing, we also then lemmatize all the words in the reviews to produce lemmatized strings.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCPc1r2yjLDD"},"outputs":[],"source":["\n","\n","def preprocess_text(text):\n","    \"\"\"Preprocesses a given string text input\"\"\"\n","    preprocs = [\n","        gpp.strip_tags, \n","        gpp.strip_punctuation,\n","        gpp.strip_multiple_whitespaces,\n","        gpp.strip_numeric,\n","        gpp.remove_stopwords, \n","        gpp.strip_short, \n","    ]\n","    text = gu.to_unicode(text.lower().strip())\n","    for preproc in preprocs:\n","        text = preproc(text)\n","    return text\n","\n","def lemmatize(text):\n","    \"\"\"Lemmatizes a given string text input\"\"\"\n","    wnl = WordNetLemmatizer()\n","    return wnl.lemmatize(text)  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7C7HKSGxjW9o"},"outputs":[],"source":["# Combining both the above functions into a single preprocessing function\n","preprocess = lambda text: lemmatize(preprocess_text(str(text)))"]},{"cell_type":"markdown","metadata":{"id":"R-eSjOdGjesy"},"source":["\n","\n","Before we apply the preprocessing, we notice that the dataset has two columns with textial data: the title of the review and the review itself. As the title of the data also indicates the feelings of the user towards the product and is essentially a summarization of the review it is also informative for predicting user rating. Therefore, we create a new feature \"ReviewFull\" which is a concatenation of the review title as well as the review itself, and use this as our primary data for EDA and model training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeGCfKQ6jZMO"},"outputs":[],"source":["# Create the ReviewFull data column\n","data[\"ReviewFull\"] = data[\"Title\"] + \" \" + data[\"Review\"]\n","data = data.drop([\"Title\", \"Review\"], axis=1)\n","\n","# Apply the preprocessing to the textual data\n","data[\"ReviewFull\"] = data[\"ReviewFull\"].apply(preprocess)\n","data.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xO68Mi21kRcN"},"outputs":[],"source":["# Save the data\n","data.to_csv(join(data_dir, \"preprocessed_train.csv\"))"]},{"cell_type":"markdown","metadata":{"id":"8wWZtzVskVLw"},"source":["\n","## Downsampling\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QGx9yz7MkXbj"},"source":["\n","We now create a smaller dataset which my computer can process when doing EDA and modeling. We downsample to a dataset size of 50000 data points, ensuring that there is an even distribution of ratings by grouping by the \"Rating\" column when sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPK2rkrVjbSP"},"outputs":[],"source":["\n","\n","downsampled = data.groupby(\"Rating\").sample(10000)\n","display(downsampled)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfPskIPQkaJz"},"outputs":[],"source":["# Ensure equal distribution of targets\n","downsampled[\"Rating\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHdchnJWkn-Y"},"outputs":[],"source":["# Save data\n","downsampled.to_csv(join(data_dir, \"downsampled_preprocessed_train_50000.csv\"))"]},{"cell_type":"markdown","metadata":{"id":"DLVS771IlEh_"},"source":["\n","# Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"6EoryDJclHpb"},"source":["Exploratory data analysis to investigate the nature of the textual features and discover potential relationships between the features and the target variable that could aid in the prediction of ratings.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSuUI_RlkqVw"},"outputs":[],"source":["# EDA related imports\n","from collections import Counter\n","from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import NMF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HOvFV_MlQJs"},"outputs":[],"source":["# Load the downsampled preprocessed data\n","data = pd.read_csv(join(data_dir, \"downsampled_preprocessed_train_50000.csv\"))[[\"Rating\", \"ReviewFull\"]]\n","data[\"ReviewFull\"] = data[\"ReviewFull\"].apply(str)\n","display(data)"]},{"cell_type":"markdown","metadata":{"id":"A2b6kmkRl12-"},"source":["\n","## Sentiment Polarity - Rating Analysis\n","\n","First we attempt to find whether or not any relationship exists between the sentiment polarity of a review and the rating. We do so by observing how to average sentiment of reviews changes with rating\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMkx6qielq0A"},"outputs":[],"source":["# Define function to extract a string's sentiment\n","find_sentiment = lambda text: TextBlob(text).sentiment.polarity\n","\n","# Create new column in dataframe\n","data[\"sentiment\"] = data[\"ReviewFull\"].apply(find_sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMUnAKV8l9XL"},"outputs":[],"source":["# Plot mean sentiment against rating \n","for rating in range(1,6):\n","    print(f\"Mean Sentiment for Rating {rating}: {data[data['Rating'] == rating]['sentiment'].mean()}\")\n","data.groupby(\"Rating\")[\"sentiment\"].mean().plot(kind=\"bar\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRZgCblomAhT"},"outputs":[],"source":["plt.scatter(data[\"sentiment\"], data[\"Rating\"])\n","plt.xlabel(\"Sentiment\")\n","plt.ylabel(\"Rating\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SR2Y65HGmLcY"},"source":["As we can see from the above plot, though we see a positive correlation with the mean sentiment of the reviews, when looking at the data at an individual level, we see that there is a lot of variance and overlap between the sentiment values of reviews of different ratings. We can investigate further by seeing what the most common words that contirbute to this sentiment are."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQ9zr5gamF8u"},"outputs":[],"source":["\n","\n","# Function for finding n nost common words in a series\n","num_most_common = 25\n","find_n_most_common = lambda text, n: Counter(' '.join(text).split()).most_common(n)\n","most_common = data.groupby(\"Rating\")[\"ReviewFull\"].apply(lambda x: find_n_most_common(x, num_most_common))\n","\n","for rating in range(1,6):\n","    idx = rating - 1\n","    print(f\"Most Common Words for Rating {rating} sorted by sentiment: \")   \n","    # Display the most common words in each rating sorted by sentiment \n","    print(list(sorted(most_common.iloc[idx], key=lambda x: TextBlob(x[0]).sentiment.polarity)))\n","    print(\"\\n\")\n","\n","    \n","\n"]},{"cell_type":"markdown","metadata":{"id":"a7nW1-GFmVBm"},"source":["\n","\n","From the above cell, we see the lower rating reviews have many occurences of negative words such as \"bad\", \"waste\", and \"terrible\" whereas higher rating reviews have many occurences of positive words such as \"best\" and \"excellent\". However, we see that there are some words of the same sentiment that occur numerous times in reviews from all the ratings, like \"better\" and \"good\".\n","\n","When finding the sentiment of a sentence using TextBlob as done above, the sentiment is calculated by a simple averaging of the sentiments of the individual words of the sentence. However, this approach does not accurately represent the differences in sentiment of two senetences that use similar words but in different contexts.\n","\n","For instance, the sentence \"the movie was better than most other\" and the sentence \"the movie could have been much better\" both use similar words such as \"better\" but have completely different sentiments.\n","\n","This dicovery shows us that when conducting sentiment analysis and attempting to predict ratings, we must use a encoding of the review which captures its contextual meaning.\n"]},{"cell_type":"markdown","metadata":{"id":"ofRZUkK6mgPu"},"source":["\n","## Topic Modeling\n","\n","We now will attempt to use an unsupervised topic modeling approach using non-negative matrix factorization to try and categorize reviews into 5 groups based on their content and try to see whether or not the general topics of the reviews are in any way correlated with their corresponding ratings.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tztf4S7LmNbI"},"outputs":[],"source":["def display_topics(model, words, num_top_words):\n","    \"\"\"Function to display the top num_top_words topic words given an NMF model and word vocabulary\"\"\"\n","    for topic_idx, topic in enumerate(model.components_):\n","        print(\"Topic %d:\" % (topic_idx))\n","        print(\" \".join([words[i]\n","                          for i in topic.argsort()[:-num_top_words - 1:-1]]))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zFBcjPNrmpSW"},"source":["\n","\n","The train dataset we use for the NMF model will be the TF-IDF vectors of each individual review in the dataset. We restrict the max features (vocabulary size) computed by the TF-IDF vectorizer in order to reduce redundancy and for performance reasons.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eREuiQYhml_r"},"outputs":[],"source":["tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=25, max_features=5000, use_idf=True)\n","tfidf = tfidf_vectorizer.fit_transform(data[\"ReviewFull\"])\n","\n","# Create document term matrix showing the TF-IDF score for each word in each review\n","tfidf_words = tfidf_vectorizer.get_feature_names()\n","doc_term_matrix = pd.DataFrame(tfidf.toarray(), columns=list(tfidf_words))\n","doc_term_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVnn1bqqmrVv"},"outputs":[],"source":["# Fit the NMF model and generate top 5 topics\n","num_top_words = 10\n","num_topics = 5\n","nmf = NMF(n_components=num_topics, random_state=0, alpha=.1, init='nndsvd').fit(tfidf)\n","display_topics(nmf, tfidf_words, num_top_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKzbFHdUmwVx"},"outputs":[],"source":["num_top_words = 10\n","num_topics = 10\n","nmf = NMF(n_components=num_topics, random_state=0, alpha=.1, init='nndsvd').fit(tfidf)\n","display_topics(nmf, tfidf_words, num_top_words)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"09-i8Wx1np6V"},"source":["\n","# Generating BERT Embeddings\n","\n","As we found from our exploratory data analysis, the textual content and raw sentiment of the reviews is indicative of its corresponding rating. However, we found out that we cannot simply use raw sentiment as training data as it does not capture any contextual information regarding the review. Therefore, we must find a way to encode our data in a way which captures both sentiment and context.\n","\n","We can create this encoding by leveraging the power of transfer learning and using a pre-trained SOTA deep neural network model: BERT.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hb29BIP5p3W-"},"outputs":[],"source":["# !pip install tensorflow==2.4.1\n","# !pip install tensorflow-text==2.4.1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmh2N91ynrBE"},"outputs":[],"source":["# BERT Specific Imports\n","import tensorflow as tf\n","import tensorflow_hub as hub \n","import tensorflow_text as text "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1VCr1yGnrDZ"},"outputs":[],"source":["# Preprocessing layer to generate the tokenized sentences and input mask\n","bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n","# Encoder layer which generates word-level and setence-level 768-dimensional text embeddings \n","bert = hub.KerasLayer('https://tfhub.dev/google/experts/bert/wiki_books/sst2/2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_65UmSbCukyt"},"outputs":[],"source":["data_dir='/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/amazon_review_full_csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":1126,"status":"ok","timestamp":1659278960559,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"},"user_tz":-330},"id":"VPECI0xtnrFx","outputId":"3e5cf0c6-f406-48fa-dc23-f1ade068f4e9"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-073379ee-47f5-4932-b125-aaff76955636\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Rating</th>\n","      <th>Title</th>\n","      <th>Review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2831949</td>\n","      <td>1</td>\n","      <td>awkward use purchased mouse days works mouse u...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20701</td>\n","      <td>1</td>\n","      <td>unfortunatley grades william johnstone book bu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>282232</td>\n","      <td>1</td>\n","      <td>dissapointed ordered product went listen casse...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1467821</td>\n","      <td>1</td>\n","      <td>color blue clear scratched looked new solid bl...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>426502</td>\n","      <td>1</td>\n","      <td>zgun digest smith wesson reccomend book incomp...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>1163757</td>\n","      <td>5</td>\n","      <td>thich provides easy access buddhist practice b...</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>1551315</td>\n","      <td>5</td>\n","      <td>garmin strret pilot gps totally amazed great w...</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>657152</td>\n","      <td>5</td>\n","      <td>far favorite album waiting patiently album rel...</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>609342</td>\n","      <td>5</td>\n","      <td>read book blew away heard dss knew state depar...</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>1723286</td>\n","      <td>5</td>\n","      <td>cats love bought read cats like cats like want...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-073379ee-47f5-4932-b125-aaff76955636')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-073379ee-47f5-4932-b125-aaff76955636 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-073379ee-47f5-4932-b125-aaff76955636');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        Rating Title                                             Review\n","0      2831949     1  awkward use purchased mouse days works mouse u...\n","1        20701     1  unfortunatley grades william johnstone book bu...\n","2       282232     1  dissapointed ordered product went listen casse...\n","3      1467821     1  color blue clear scratched looked new solid bl...\n","4       426502     1  zgun digest smith wesson reccomend book incomp...\n","...        ...   ...                                                ...\n","49995  1163757     5  thich provides easy access buddhist practice b...\n","49996  1551315     5  garmin strret pilot gps totally amazed great w...\n","49997   657152     5  far favorite album waiting patiently album rel...\n","49998   609342     5  read book blew away heard dss knew state depar...\n","49999  1723286     5  cats love bought read cats like cats like want...\n","\n","[50000 rows x 3 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(join(data_dir, \"downsampled_preprocessed_train_50000.csv\"), names=['Rating', 'Title', 'Review'])\n","data = data.iloc[1:, :].reset_index(drop=True)\n","data[\"Rating\"] = data[\"Rating\"].apply(int) \n","data \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1659279022711,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"},"user_tz":-330},"id":"3oe8Q97EnrIF","outputId":"1ef8dca5-9fe9-428f-ee58-85d4042e73e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (7200,) | X_val: (1800,) | X_test: (1000,) | \n","y_train: (7200,) | y_val: (1800,) | y_test: (1000,) | \n"]}],"source":["# Create train, val, and test sets\n","train_data = shuffle(data)[:10000]\n","X = train_data[\"Review\"].to_numpy()\n","y = train_data[\"Rating\"].to_numpy() - 1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n","print(f\"X_train: {X_train.shape} | X_val: {X_val.shape} | X_test: {X_test.shape} | \\n\" +\n","    f\"y_train: {y_train.shape} | y_val: {y_val.shape} | y_test: {y_test.shape} | \")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3257911,"status":"ok","timestamp":1659282296393,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"},"user_tz":-330},"id":"O8dQiSI-nrJ5","outputId":"8dc27ede-ce1f-45ee-e2a1-a3b091f2204c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [54:17<00:00, 32.58s/it]\n"]},{"data":{"text/plain":["<tf.Tensor: shape=(100, 72, 768), dtype=float32, numpy=\n","array([[[-0.1948052 , -0.95507926, -0.24179976, ...,  0.05045499,\n","          0.2423901 , -0.03863095],\n","        [ 0.35723734, -0.0274303 ,  0.4144501 , ...,  0.2758953 ,\n","          0.62082785, -0.86638427],\n","        [ 0.14349917, -0.22837189, -0.23407343, ...,  0.611253  ,\n","         -0.14926855, -0.7794769 ],\n","        ...,\n","        [ 0.44140217, -0.67050815,  0.47997528, ...,  0.5887903 ,\n","         -0.13771066, -0.6668596 ],\n","        [ 0.04778063, -0.17479178, -0.5822535 , ..., -0.78787553,\n","         -0.97851884,  0.371748  ],\n","        [-0.10331483, -0.31313476, -0.6607549 , ...,  0.58495533,\n","         -0.56560117,  0.2668065 ]],\n","\n","       [[ 0.13625519, -0.75547063, -0.21902573, ..., -0.06331038,\n","         -0.6633688 , -0.03453353],\n","        [ 0.48480302,  0.11011658, -0.31587446, ..., -0.39897922,\n","          0.8270069 , -0.1672962 ],\n","        [ 0.5322556 , -0.03090397,  0.54893243, ..., -0.10614056,\n","          0.63226426, -0.7130269 ],\n","        ...,\n","        [ 0.0600886 , -0.62811726,  0.35566655, ..., -0.8466448 ,\n","         -0.9566432 ,  0.5478665 ],\n","        [ 0.03959791, -0.3011894 , -0.59771603, ..., -0.41083872,\n","         -0.8144619 ,  0.6566321 ],\n","        [ 0.09492218, -0.72323376, -0.46696383, ...,  0.6520736 ,\n","         -0.6436562 , -0.46478933]],\n","\n","       [[ 0.0107931 , -0.76966006, -0.44770387, ..., -0.43031508,\n","         -0.9383542 , -0.16359808],\n","        [-0.11427313, -0.3239461 , -0.88489074, ..., -0.00193519,\n","          0.04440987,  0.2537311 ],\n","        [-0.5960285 , -0.92751044, -0.41376197, ...,  0.31070137,\n","         -0.61870027,  0.09107534],\n","        ...,\n","        [-0.02567346, -0.46130854,  0.2603905 , ...,  0.11661749,\n","          0.14603236, -0.66227686],\n","        [ 0.124616  , -0.54786736, -0.23417675, ...,  0.13121057,\n","         -0.8786299 ,  0.03752629],\n","        [-0.41760036, -0.5091951 ,  0.05182789, ...,  0.7036983 ,\n","         -0.42273945, -0.62255925]],\n","\n","       ...,\n","\n","       [[ 0.08798256, -0.48942086, -0.65095294, ..., -0.41731027,\n","         -0.73893446, -0.01755469],\n","        [ 0.32946736,  0.13921389, -0.09491828, ...,  0.5774364 ,\n","          0.5276449 , -0.80435944],\n","        [ 0.3312362 , -0.6100971 ,  0.19397765, ..., -0.748607  ,\n","         -0.96584904,  0.57392627],\n","        ...,\n","        [ 0.20745428, -0.1847649 , -0.8168029 , ...,  0.5768593 ,\n","         -0.4256072 , -0.53315324],\n","        [ 0.1492548 , -0.63609654, -0.73235637, ..., -0.86564124,\n","         -0.8733495 ,  0.56230307],\n","        [ 0.25893036, -0.799291  ,  0.02381707, ...,  0.21201068,\n","         -0.3130847 ,  0.09347873]],\n","\n","       [[-0.02713957, -0.11088873, -0.6250474 , ..., -0.7530929 ,\n","         -0.78542054,  0.33665463],\n","        [-0.44602948, -0.89121634, -0.2358453 , ...,  0.4665986 ,\n","         -0.6746629 ,  0.10648587],\n","        [-0.17185998, -0.6127594 , -0.7916678 , ...,  0.6519421 ,\n","          0.18516627, -0.89276093],\n","        ...,\n","        [ 0.27254128, -0.66968936, -0.7599647 , ..., -0.24005853,\n","         -0.8412832 , -0.01571125],\n","        [ 0.1234985 , -0.03456322, -0.46779355, ..., -0.76570845,\n","         -0.9390356 , -0.08500599],\n","        [-0.10687992, -0.3062827 , -0.7221722 , ..., -0.23666988,\n","         -0.8656623 ,  0.5556179 ]],\n","\n","       [[-0.27406928, -0.5398159 , -0.41515568, ...,  0.00282561,\n","         -0.78357583,  0.36004972],\n","        [-0.00810447, -0.09517957, -0.1170389 , ...,  0.54706603,\n","          0.3898832 , -0.4830407 ],\n","        [ 0.41178888, -0.52543527, -0.6504361 , ..., -0.54788166,\n","         -0.89486605,  0.0155831 ],\n","        ...,\n","        [ 0.44993445, -0.39659348, -0.42990112, ...,  0.68515074,\n","         -0.05103956, -0.7578401 ],\n","        [ 0.02418073, -0.86086106, -0.14164042, ...,  0.06431253,\n","         -0.8444699 , -0.60928935],\n","        [-0.69640225, -0.7164207 , -0.5745411 , ...,  0.6731318 ,\n","          0.03792674, -0.0730956 ]]], dtype=float32)>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Generate train embeddings\n","def generate_bert_embeddings(data):\n","    \"\"\"Generate the BERT embeddings for a given Series/list of senetences\"\"\"\n","    return bert(bert_preprocess(data))['pooled_output'] \n","\n","def generate_embeddings_list(data):\n","    \"\"\"Generate embeddings of parts of list individually and then concatenate. \n","    Create to overcome performance issues.\"\"\"\n","    factor = int(data.shape[0]/100)\n","    embeddings_list = []\n","    for i in tqdm(range(0, 100)):\n","        embeddings_list.append(generate_bert_embeddings(X[factor*i: factor*(i+1)]))\n","    return embeddings_list\n","    \n","el = generate_embeddings_list(X_train)\n","embeddings = tf.stack(el)\n","embeddings\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mriHK-k_vMwi"},"outputs":[],"source":["embeddings = tf.reshape(embeddings, (7200, 768))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"II9GV44pvMzJ"},"outputs":[],"source":["# Save train embeddings\n","import pickle\n","pickle.dump(embeddings, open(join(data_dir, \"downsampled_shuffled_train_embeddings.pkl\"), \"wb\"))\n","pickle.dump(y_train, open(join(data_dir, \"downsampled_shuffled_train_labels.pkl\"), \"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1361413,"status":"ok","timestamp":1659283657795,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"},"user_tz":-330},"id":"QXfM8u65vM04","outputId":"f5c0499a-a48d-483d-aa0e-9ddf3c02cd02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating val data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [14:16<00:00,  8.56s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Generating test data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [08:24<00:00,  5.05s/it]\n"]}],"source":["\n","\n","# Generate and save validation and test embeddings \n","print(\"Generating val data...\")\n","val_embeddings = tf.reshape(tf.stack(generate_embeddings_list(X_val)), (1800, 768))\n","print(\"Generating test data...\")\n","test_embeddings = tf.reshape(tf.stack(generate_embeddings_list(X_test)), (1000, 768))\n","\n","pickle.dump(val_embeddings, open(join(data_dir, \"downsampled_shuffled_val_embeddings.pkl\"), \"wb\"))\n","pickle.dump(y_val, open(join(data_dir, \"downsampled_shuffled_val_labels.pkl\"), \"wb\"))\n","\n","pickle.dump(test_embeddings, open(join(data_dir, \"downsampled_shuffled_test_embeddings.pkl\"), \"wb\"))\n","pickle.dump(y_test, open(join(data_dir, \"downsampled_shuffled_test_labels.pkl\"), \"wb\"))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-qXG7sC5m-qc"},"source":["\n","# Classical ML Models Benchmark\n","\n","This notebook contains attempts to solve the problem of predicting ratings will classical ML models which support multinomial classification. The scores achieved by these models will serve as a benchmark for the deep neural network based approach\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5T4UtPqm7M4"},"outputs":[],"source":["# Classifier Imports\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV, KFold\n","\n","\n","SEED = 0\n","CPU_COUNT = multiprocessing.cpu_count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1660478766858,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"},"user_tz":-330},"id":"WYN2xNrDY__t","outputId":"448ede13-229f-4540-ef03-7a032ad8d50a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":7}],"source":["CPU_COUNT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiUolab5nENx"},"outputs":[],"source":["# Wrapper class for a general SKLearn classifier\n","class Classifier():\n","    def __init__(self, classifier_name, classifier, init_params, param_grid, seed):\n","        self.classifier_name = classifier_name\n","        self.seed = seed\n","        self.param_grid = param_grid\n","\n","        #Init classifier\n","        self.init_params = init_params\n","        self.init_params[\"random_state\"] = seed\n","        self.classifier = classifier(**self.init_params) if init_params else classifier(random_state=seed)\n","\n","        #Dict to explicitly store best stats\n","        self.best_stats = {\"best_params\": None, \"best_score\": None}\n","    \n","    def fit(self, X, y):\n","        print(f\"Fitting {self.classifier_name} model...\")\n","        self.classifier.fit(X, y)\n","\n","    def predict(self, X):\n","        return self.classifier.predict(X)\n","    \n","    def evaluate(self, X_test, y_test):\n","        return self.classifier.score(X_test, y_test)\n","    \n","    def tune_hyperparameters(self, X, y):\n","        print(f\"Tuning hyperparameters for {self.classifier_name} model...\")\n","        cv = KFold(n_splits=5, random_state=self.seed, shuffle=True)\n","        gscv = GridSearchCV(self.classifier, self.param_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n","        gscv.fit(X, y)\n","        self.classifier = gscv.best_estimator_\n","        self.best_stats[\"best_params\"], self.best_stats[\"best_score\"] = gscv.best_params_, gscv.best_score_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5530,"status":"ok","timestamp":1660478779419,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"},"user_tz":-330},"id":"UTwd3dUEnT7P","outputId":"a21d0f64-c80d-4fd1-df25-85dc07ff1fb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train: (9000, 768) | X_test: (1000, 768) | \n","y_train: (9000,) | y_test: (1000,) | \n"]}],"source":["data_dir = \"/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/amazon_review_full_csv\"\n","\n","\n","# Loading train, val, and test data (BERT text embeddings and corresponding labels)\n","\n","X_train = pickle.load(open(join(data_dir, \"downsampled_shuffled_train_embeddings.pkl\"), \"rb\")).numpy()\n","y_train = pickle.load(open(join(data_dir, \"downsampled_shuffled_train_labels.pkl\"), \"rb\"))\n","X_val = pickle.load(open(join(data_dir, \"downsampled_shuffled_val_embeddings.pkl\"), \"rb\")).numpy()\n","y_val = pickle.load(open(join(data_dir, \"downsampled_shuffled_val_labels.pkl\"), \"rb\"))\n","X_test = pickle.load(open(join(data_dir, \"downsampled_shuffled_test_embeddings.pkl\"), \"rb\")).numpy()\n","y_test = pickle.load(open(join(data_dir, \"downsampled_shuffled_test_labels.pkl\"), \"rb\"))\n","\n","# Combine train and validation set into one as we use K-Fold cross validation\n","X_train = np.concatenate([X_train, X_val])\n","y_train = np.concatenate([y_train, y_val])\n","\n","print(f\"X_train: {X_train.shape} | X_test: {X_test.shape} | \\n\" +\n","    f\"y_train: {y_train.shape} | y_test: {y_test.shape} | \")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YK5ZjhmvnV0Z"},"outputs":[],"source":["# Create list of classifiers\n","SEED = 0\n","param_grids = []\n","\n","# Create parameter grids for hyperparameter tuning\n","rf_param_grid = {\"max_features\": [\"sqrt\", \"log2\"],\n","                    \"max_depth\" : [3, 6, 8],\n","                    \"criterion\" :[\"gini\", \"entropy\"]     ,\n","                    \"n_jobs\": [-1]}\n","\n","lsvc_param_grid = {\"penalty\": [\"l2\"],\n","                   \"C\": [0.0001, 0.01, 1.0, 10, 100]}\n","\n","lreg_param_grid = {'penalty' : ['l1', 'l2'],\n","                     'C' : np.logspace(-4, 4, 20)}\n","\n","clf_names = [\"RandomForest\", \"LinearSVC\", \"LogisticRegression\"]\n","clfs = [RandomForestClassifier, LinearSVC, LogisticRegression]\n","init_params = [{'n_jobs': CPU_COUNT}, {'multi_class': 'crammer_singer'}, {'multi_class': 'multinomial', 'solver': 'lbfgs'}]\n","param_grids.extend([rf_param_grid, lsvc_param_grid, lreg_param_grid])\n","\n","\n","classifiers = [Classifier(name, model, {}, param_grid, SEED) for name, model, param_grid in zip(clf_names, clfs, param_grids)]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAU92XdJvltH","outputId":"ec9b67b4-5a91-4ffa-e921-fd4d83eb9c82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting RandomForest model...\n"]}],"source":["[clf.fit(X_train, y_train) for clf in classifiers]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niIj7NWo4HVl"},"outputs":[],"source":["# Score classifier\n","[clf.evaluate(X_test, y_test) for clf in classifiers]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KhDf_B34JhW"},"outputs":[],"source":["# Tune hyperparameters\n","[clf.tune_hyperparameters(X_train, y_train) for clf in classifiers]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBCVWp8s4LSc"},"outputs":[],"source":["# Score tuned clasifiers\n","[clf.evaluate(X_test, y_test) for clf in classifiers]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAzEVWE34Nno"},"outputs":[],"source":["# Save models\n","models_dir = \"models/\"\n","for clf in classifiers:\n","    pickle.dump(clf, open(join(models_dir, f\"{clf.classifier_name}.pkl\"), \"wb\")) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-EjYOp-4QII"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# BERT MODEL"],"metadata":{"id":"gf6h_YmKQWzj"}},{"cell_type":"code","source":["pip install tensorflow-text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ye_DxtIvQt8U","executionInfo":{"status":"ok","timestamp":1660479083806,"user_tz":-330,"elapsed":76386,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"07d0d6a5-3afa-4799-b926-72effa5990d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[K     |████████████████████████████████| 4.6 MB 21.2 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n","Collecting tensorflow<2.10,>=2.9.0\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |████████████████████████████████| 511.7 MB 6.5 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.1.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.47.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n","Collecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 60.1 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 74.4 MB/s \n","\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (57.4.0)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.5.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, tensorflow-text\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["flatbuffers","gast","keras","tensorboard","tensorflow","tensorflow_estimator"]}}},"metadata":{}}]},{"cell_type":"code","source":["# General Imports\n","import pandas as pd \n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from os.path import join\n","\n","# NN-related imports\n","import tensorflow as tf\n","import tensorflow_hub as hub \n","import tensorflow_text as text \n","\n","print(tf.test.is_built_with_cuda())\n","print(tf.config.list_physical_devices('GPU'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_Oe5AMkQZsM","executionInfo":{"status":"ok","timestamp":1660479115848,"user_tz":-330,"elapsed":3386,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"530289ae-00ef-4b39-fc3b-0c63840e2bec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"code","source":["bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n","bert = hub.KerasLayer('https://tfhub.dev/google/experts/bert/wiki_books/sst2/2')"],"metadata":{"id":"rJVehU7JQdOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = \"/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/amazon_review_full_csv\"\n","data = pd.read_csv(join(data_dir, \"downsampled_preprocessed_train_50000.csv\"))[[\"Rating\", \"ReviewFull\"]]\n","data[\"ReviewFull\"] = data[\"ReviewFull\"].apply(str)\n","display(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"XmTg_BhuQfwb","executionInfo":{"status":"ok","timestamp":1660479350071,"user_tz":-330,"elapsed":986,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"e9334c81-6095-4eaa-bd96-e545a0959c45"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["       Rating                                         ReviewFull\n","0           1  awkward use purchased mouse days works mouse u...\n","1           1  unfortunatley grades william johnstone book bu...\n","2           1  dissapointed ordered product went listen casse...\n","3           1  color blue clear scratched looked new solid bl...\n","4           1  zgun digest smith wesson reccomend book incomp...\n","...       ...                                                ...\n","49995       5  thich provides easy access buddhist practice b...\n","49996       5  garmin strret pilot gps totally amazed great w...\n","49997       5  far favorite album waiting patiently album rel...\n","49998       5  read book blew away heard dss knew state depar...\n","49999       5  cats love bought read cats like cats like want...\n","\n","[50000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-3de23bb8-763d-485c-988b-c5eaf77c3686\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Rating</th>\n","      <th>ReviewFull</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>awkward use purchased mouse days works mouse u...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>unfortunatley grades william johnstone book bu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>dissapointed ordered product went listen casse...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>color blue clear scratched looked new solid bl...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>zgun digest smith wesson reccomend book incomp...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>5</td>\n","      <td>thich provides easy access buddhist practice b...</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>5</td>\n","      <td>garmin strret pilot gps totally amazed great w...</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>5</td>\n","      <td>far favorite album waiting patiently album rel...</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>5</td>\n","      <td>read book blew away heard dss knew state depar...</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>5</td>\n","      <td>cats love bought read cats like cats like want...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3de23bb8-763d-485c-988b-c5eaf77c3686')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3de23bb8-763d-485c-988b-c5eaf77c3686 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3de23bb8-763d-485c-988b-c5eaf77c3686');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["\n","\n","def build_model():\n","    \"\"\"Build model with custom classifier stacked on top of BERT encoder\"\"\"\n","    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='txt_input')\n","    bert_input = bert_preprocess(text_input)\n","    bert_output = bert(bert_input)\n","    clf_input = bert_output['pooled_output']\n","    clf = tf.keras.layers.Dropout(0.1)(clf_input) \n","    clf = tf.keras.layers.Dense(384, activation='sigmoid', kernel_regularizer='l2')(clf)\n","    clf = tf.keras.layers.Dropout(0.1)(clf)\n","    clf = tf.keras.layers.Dense(5, activation='sigmoid', name='clf')(clf)\n","    return tf.keras.Model(text_input, clf)\n","\n","model = build_model() \n","\n"],"metadata":{"id":"D2jUVpT8RKcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2B63-CssRRQH","executionInfo":{"status":"ok","timestamp":1660479358987,"user_tz":-330,"elapsed":710,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"f784c637-a844-41bb-b502-d78f8c749344"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," txt_input (InputLayer)         [(None,)]            0           []                               \n","                                                                                                  \n"," keras_layer (KerasLayer)       {'input_mask': (Non  0           ['txt_input[0][0]']              \n","                                e, 128),                                                          \n","                                 'input_type_ids':                                                \n","                                (None, 128),                                                      \n","                                 'input_word_ids':                                                \n","                                (None, 128)}                                                      \n","                                                                                                  \n"," keras_layer_1 (KerasLayer)     {'default': (None,   109482241   ['keras_layer[0][0]',            \n","                                768),                             'keras_layer[0][1]',            \n","                                 'pooled_output': (               'keras_layer[0][2]']            \n","                                None, 768),                                                       \n","                                 'encoder_outputs':                                               \n","                                 [(None, 128, 768),                                               \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768)],                                               \n","                                 'sequence_output':                                               \n","                                 (None, 128, 768)}                                                \n","                                                                                                  \n"," dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n","                                                                                                  \n"," dense (Dense)                  (None, 384)          295296      ['dropout[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 384)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," clf (Dense)                    (None, 5)            1925        ['dropout_1[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,779,462\n","Trainable params: 297,221\n","Non-trainable params: 109,482,241\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metrics = [tf.metrics.SparseCategoricalAccuracy()]\n","optimizer = tf.keras.optimizers.Adam()\n","\n","model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"],"metadata":{"id":"z0u002jvRTvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = shuffle(data)[:10000]\n","X = train_data[\"ReviewFull\"].to_numpy()\n","y = train_data[\"Rating\"].to_numpy() - 1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n","print(f\"X_train: {X_train.shape} | X_val: {X_val.shape} | X_test: {X_test.shape} | \\n\" +\n","    f\"y_train: {y_train.shape} | y_val: {y_val.shape} | y_test: {y_test.shape} | \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIu9OAHeRV7k","executionInfo":{"status":"ok","timestamp":1660479391756,"user_tz":-330,"elapsed":876,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"c6c49412-d174-4d5c-c062-9e4c66c5cd16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train: (7200,) | X_val: (1800,) | X_test: (1000,) | \n","y_train: (7200,) | y_val: (1800,) | y_test: (1000,) | \n"]}]},{"cell_type":"code","source":["EPOCHS = 25\n","BATCH_SIZE = 64\n","history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val))\n"],"metadata":{"id":"vrdzD_ytRX2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.saved_model.save(model, \"models/bert2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xmySOofSPJW","executionInfo":{"status":"ok","timestamp":1660480753641,"user_tz":-330,"elapsed":13820,"user":{"displayName":"Ayush Mishra","userId":"06734943114352523111"}},"outputId":"e3b45977-b583-457e-dca7-942b9fafaef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 360). These functions will not be directly callable after loading.\n"]}]},{"cell_type":"code","source":["model = tf.keras.models.load_model(\"models/bert2\")"],"metadata":{"id":"_l4DFXRQSSGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(X_test, y_test)"],"metadata":{"id":"8dzAzjuIST3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jWr7pDoHSWBv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Sentiment_ANALYSIS.ipynb","provenance":[],"collapsed_sections":["DLVS771IlEh_","A2b6kmkRl12-","ofRZUkK6mgPu","09-i8Wx1np6V"],"mount_file_id":"1RjUbJIevxFLeKzQZkWp1a43_fXPqgVIn","authorship_tag":"ABX9TyPy/XnEZ56wge7m2YbGVgY/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}